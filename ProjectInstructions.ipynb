{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project guidelines\n",
    "\n",
    "For the final assessment of our course, you are required to complete a small project.\n",
    "\n",
    "Choose one project topic from the following list:\n",
    "* Categorising data: supervised and unsupervised learning in Python\n",
    "* Finite difference method for the heat equation\n",
    "* Newton and Broyden's methods for nonlinear systems\n",
    "* Polynomial least squares\n",
    "* Loss of significance in floating-point arithmetic\n",
    "* Writing a Python tutorial\n",
    "\n",
    "Each topic is introduced in more detail in the following sections. A project outline is given for each topic, including a description of the objectives and guidelines to get you started. The projects are designed to be open-ended -- you can take them in any direction you like. I'll be happy to provide further guidance at any point if necessary.\n",
    "\n",
    "### Choosing your topic\n",
    "\n",
    "* If you have previous programming experience, you may wish to choose a topic with which you are *not* particularly familiar, if anything to challenge yourself. Alternatively, you might want to revisit some of your past work, or a result or algorithm from the literature, and try reproducing it in Python.\n",
    "\n",
    "* If you are involved or interested in teaching, I would **strongly** recommend the 'Writing a Python tutorial' topic. Setting aside the most obvious and direct purpose of practising writing teaching material (and more specifically designing Jupyter notebooks to present programming content), I would argue that finding a neat way to explain/demonstrate how something works is a fantastic way to learn a *lot* about that thing. If there is another topic that interests you, you could write a tutorial about that topic.\n",
    "\n",
    "* For most of these topics, there are multiple possible directions of investigation. The choice of direction is yours -- the important thing is that **your work clearly demonstrates your Python skills**. Some of these directions may be, for example:\n",
    "    * implement an algorithm from scratch, test and investigate different use cases or parameters, try to optimise it as much as possible\n",
    "    * implement two or more algorithms, investigate and compare their behaviour under similar conditions\n",
    "    * use existing implementations (e.g. the algorithms in the `scikit-learn` module for machine learning), and focus on investigating, for instance, performance and/or efficiency\n",
    "    * focus on a simple case, and produce interesting/insightful visualisations using Python tools\n",
    "    \n",
    "### Citing sources appropriately\n",
    "\n",
    "The **vast majority** of the code in your project should be authored by yourself. That being said, while researching your topic, it is very likely that you will come across some helpful resources -- you may even find a piece of code, written and shared by someone else, which solves a part of your problem.\n",
    "\n",
    "Citing sources and referencing other people's work appropriately is **just as important for code as it is for any other academic work**. You can of course take a look at a code snippet you have found online for inspiration, or even directly adapt a **small** piece of code (anything short of copy/pasting is acceptable); but in all cases, you must provide **references** to any such sources.\n",
    "\n",
    "You can provide references for books, articles, notes, documentation, etc. as usual, in a Markdown cell (you can have a list of references at the end of your notebook, for instance). If you have found the resource online, provide the URL to access it, the author (if that information is available), and the date accessed. Forum answers (e.g. from StackOverflow/StackExchange) should also be referenced if they are used.\n",
    "\n",
    "Any piece of **code** which needs to be referenced should be **tagged** between specific code comments, as follows (you can copy/paste the template and adapt it):\n",
    "\n",
    "```python\n",
    "### REFERENCE xx\n",
    "[your code]\n",
    "### END REFERENCE xx\n",
    "```\n",
    "where `xx` should be replaced by a number or tag, corresponding to the entry in your list of references.\n",
    "\n",
    "As a general rule of thumb, projects which don't cite *any* sources are unlikely to receive a passing grade.\n",
    "\n",
    "### Submitting your project\n",
    "\n",
    "Once you have chosen a topic, simply **edit the empty `Project.ipynb` notebook** to add your content. Submit your completed `Project.ipynb` **on Noteable** before **Wednesday 21st October 2020, 11.59pm**.\n",
    "\n",
    "Follow the instructions on the [Noteable documentation](https://noteable.edina.ac.uk/student_guide/) to submit your assignment. Your feedback will also be released on Noteable, as described there.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project A: Categorising data: supervised and unsupervised learning in Python\n",
    "\n",
    "In this project, you will investigate one or both of the following machine learning problems:\n",
    "\n",
    "- **Classification** is a *supervised* learning problem, where all items in a data set are classified into two or more labelled categories. The number of categories and their labels are known in advance.\n",
    "- **Clustering** is an *unsupervised* learning problem, where the data set is partitioned into a number of *clusters*, based on the similarity between items. The number of categories or labels are *not* known in advance.\n",
    "\n",
    "The aim of the project is to perform a classification or clustering task, using at least one method, on at least one dataset. In your notebook, you should:\n",
    "* **present the task**: describe the dataset you are investigating, and outline the aims/objectives of the problem. What question are you trying to answer?\n",
    "* **choose a method**: describe any algorithm(s) you are using to solve the problem, and explain why they are suitable for the task.\n",
    "* **implement the method**: write code to compute solutions to your problem. Your code should be well-commented and sensibly organised. Your code should also include *any* setup and/or pre-processing you perform on the data (for example, extracting the .csv file into a numeric array, dividing the data into a training set and a testing set...). Don't hesitate to write functions to encapsulate the different subtasks.\n",
    "* **visualise and evaluate the results**: first, think about what quantities you could compute to evaluate the performance of your method. After that, there are many things you could choose to investigate, for instance:\n",
    "    * How does the performance of your method seem to change with different parameters or datasets?\n",
    "    * If you are investigating multiple methods, or multiple variants of the same method, does one generally seem to perform better or worse than others? Why do you think this may be?\n",
    "    * Can you find ways to visualise your results, to better understand them?\n",
    "\n",
    "### Implementation\n",
    "\n",
    "A good reference for machine learning methods is the book [*Elements of Statistical Learning*, by Hastie, Tibshirani, and Friedman](https://web.stanford.edu/~hastie/ElemStatLearn/). You should be able to find all the mathematical background needed for a wide variety of statistical learning methods, and plenty of ideas for evaluation and further investigation.\n",
    "\n",
    "The [scikit-learn module](https://scikit-learn.org/stable/) provides a number of algorithms for classification and clustering problems, as well as tools for pre-processing data. You may wish to use these for your project, for instance if you want to investigate the performance of different algorithms with a given dataset, or to evaluate the usefulness of different pre-processing methods. Start with one of the [tutorial examples](https://scikit-learn.org/stable/tutorial/basic/tutorial.html) to get to grips with the module's functionality.\n",
    "\n",
    "Alternatively, you may wish to implement an algorithm from first principles to solve either of these problems. In this case, I would probably recommend a classification algorithm which is relatively straightforward to implement: the *$k$-nearest-neighbours* algorithm.\n",
    "\n",
    "#### $k$-nearest-neighbours\n",
    "\n",
    "The $k$-nearest-neighbours algorithm is a classic machine learning algorithm used for classification problems. The data is assumed to be separated into a *training dataset*, where the class (the label) of each item is known, and a *test dataset*, which contains the unlabelled data that you would like to classify.\n",
    "\n",
    "A basic implementation is as follows: for each item in the test dataset, find the $k$ nearest items in the training dataset -- these are your $k$ nearest neighbours. The class of your unlabelled item is then determined by majority voting amongst the classes of these $k$ neighbours. In the case where two or more classes are tied in the vote, the tie is resolved by taking the class of the nearest data item in the training dataset.\n",
    "\n",
    "There are 3 different aspects of this algorithm which you have control over:\n",
    "* $k$, the number of nearest neighbours,\n",
    "* the *distance metric*, i.e. what is meant by \"nearest\" -- you could start with computing the Euclidean distance between items, and then experiment with other metrics,\n",
    "* the voting method -- majority voting is not the only possibility; for instance, you could weigh the votes according to distances, so that the closest of the $k$ neighbours have more influence on the classification of the test item.\n",
    "\n",
    "### Datasets\n",
    "\n",
    "[This archive](https://archive.ics.uci.edu/ml/datasets.php) provides hundreds of different data sets which you can use for your project. Different datasets are suited for different machine learning problems -- you will be able to find many different appropriate datasets for both classification and clustering. If you wish to implement an algorithm from first principles, you may wish to use a fairly simple dataset -- a couple of suggestions:\n",
    "* [The iris dataset](https://archive.ics.uci.edu/ml/datasets/Iris) is a small but very well-known dataset, containing 150 labelled data points, each with 4 attributes representing 4 different measurements on iris flowers. Each data point belongs to one of three species of iris flower.\n",
    "* [The MNIST dataset](http://yann.lecun.com/exdb/mnist/) is much larger, and also very well-known. Each data point is a $28\\times 28$ pixel greyscale image of a handwritten digit (0-9), each with 784 attributes (the intensity of each pixel). Each data point (in the training set) is labelled with the correct number.\n",
    "* [The wine dataset](http://archive.ics.uci.edu/ml/datasets/Wine) contains the results of chemical analysis of 178 different wines, made with grapes from one of three possible types of vine.\n",
    "* scikit-learn comes with a few [built-in datasets](https://scikit-learn.org/stable/datasets/index.html#toy-datasets), including iris and wine.\n",
    "* Alternatively, particularly for clustering problems, you may wish to generate your own synthetic data -- again, you can do this [with scikit-learn](https://scikit-learn.org/stable/datasets/index.html#generated-datasets).\n",
    "\n",
    "### Resources\n",
    "\n",
    "* [*Elements of Statistical Learning*, by Hastie, Tibshirani, and Friedman](https://web.stanford.edu/~hastie/ElemStatLearn/) -- a solid textbook reference.\n",
    "* [Start Here With Machine Learning](https://machinelearningmastery.com/start-here/) -- a series of guides and tutorials at different levels.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project B: Finite difference method for the heat equation\n",
    "\n",
    "In this project, you will produce simulations of the evolution of the temperature distribution in a medium, by computing numerical solutions to the heat equation using the **finite difference method**, under different initial and boundary conditions.\n",
    "\n",
    "### The 1D heat equation\n",
    "\n",
    "Let the temperature distribution over a thin rod of length $L$ be defined as $u(x, t)$, where $t\\geq 0$ and $x \\in [0, L]$. The evolution of this temperature distribution is governed by the *1D heat equation*,\n",
    "\n",
    "$$\n",
    "\\frac{\\partial u}{\\partial t} = \\alpha \\frac{\\partial^2 u}{\\partial x^2},\n",
    "$$\n",
    "\n",
    "where $\\alpha$ is the thermal diffusivity of the medium. The full initial-boundary value problem is defined by this equation, plus an initial condition $u(x, 0) = f(x)$, representing the temperature distribution along the rod at time $t=0$, and two boundary conditions, one at each end of the rod. For example, the temperature may be fixed at 0 at both ends, i.e.\n",
    "\n",
    "$$\n",
    "u(0, t) = u(L, t) = 0.\n",
    "$$\n",
    "\n",
    "### Discretisation\n",
    "\n",
    "The idea behind finite difference methods is to compute approximated solutions to an initial-boundary value problem like this one, by discretising the domain of definition of $u$ and approximating partial derivative operators with difference operators.\n",
    "\n",
    "Let $\\Delta t >0$ and $\\Delta x >0$ denote temporal and spatial step sizes, respectively. Consider now a discrete function $u_l^n$, defined over $n \\in \\{0, 1, 2, \\dots\\}$ and $l \\in \\{0, \\dots, N\\}$ (where $N=\\frac{L}{\\Delta x}$), such that $u_l^n$ is an approximation of the temperature distribution $u$ at time $t_0=n\\Delta t$ and $x_0 = l\\Delta x$, that is\n",
    "$$\n",
    "u_l^n \\approx u(l\\Delta x, n\\Delta t) = u(x_0, t_0).\n",
    "$$\n",
    "\n",
    "We therefore have, for example,\n",
    "$$\n",
    "\\begin{align}\n",
    "u_{l+1}^n &\\approx u((l+1)\\Delta x, n\\Delta t) = u(x_0 + \\Delta x, t_0), \\\\\n",
    "u_l^{n-1} &\\approx u(l\\Delta x, (n-1)\\Delta t) = u(x_0, t_0 - \\Delta t).\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Partial derivatives may be approximated by finite differences. To see this, take the definition of the derivative:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial u}{\\partial t}(x, t_0) = \\lim_{\\Delta t \\to 0} \\frac{u(x, t_0 + \\Delta t) - u(x, t_0)}{\\Delta t}.\n",
    "$$\n",
    "\n",
    "A finite difference approximation of $\\frac{\\partial u}{\\partial t}$ can be defined by letting $\\Delta t$ take a small, finite value:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial u}{\\partial t}(x, t_0) \\approx \\frac{u(x, t_0 + \\Delta t) - u(x, t_0)}{\\Delta t}\n",
    "\\approx \\frac{u_l^{n+1} - u_l^n}{\\Delta t}.\n",
    "$$\n",
    "\n",
    "This is the *forward* temporal difference, since it requires an approximation of $u(x, t)$ at a point *forward* in time, $t_0 + \\Delta t$.\n",
    "\n",
    "Similarly, a *centred* finite difference approximation of $\\frac{\\partial^2 u}{\\partial x^2}$ is given by\n",
    "\n",
    "$$\n",
    "\\frac{\\partial^2 u}{\\partial x^2} \\approx \\frac{u_{l-1}^n - 2u_l^n + u_{l+1}^n}{(\\Delta x)^2}.\n",
    "$$\n",
    "\n",
    "Using these approximations, we can write a finite difference *scheme* to approximate the 1D heat equation -- the *forward-time, centred-space (FTCS) scheme*:\n",
    "\n",
    "$$\n",
    "\\frac{u_l^{n+1} - u_l^n}{\\Delta t} = \\alpha \\frac{u_{l-1}^n - 2u_l^n + u_{l+1}^n}{(\\Delta x)^2}.\n",
    "$$\n",
    "\n",
    "### Computing numerical solutions\n",
    "\n",
    "We can use our FTCS finite difference scheme to compute numerical solutions to the heat equation. The idea is to rearrange it into a *recursion* in time, so that the temperature distribution at a given time step can be directly computed from the distribution at previous time steps. The FTCS scheme can be rearranged to give $u_l^{n+1}$ in terms of $u_{l-1}^n$, $u_l^n$, and $u_{l+1}^n$, all known from the previous step:\n",
    "\n",
    "$$\n",
    "u_l^{n+1} = \\frac{\\alpha \\Delta t}{(\\Delta x)^2} \\left(u_{l+1}^n + u_{l-1}^n\\right)\n",
    "+ \\left(1 - \\frac{2\\alpha \\Delta t}{(\\Delta x)^2}\\right) u_l^n.\n",
    "$$\n",
    "\n",
    "The initial condition gives the temperature distribution at initial time -- that is, we can set $u_l^0 = u(l\\Delta x, 0)$. The temperature distribution at future time steps can now be computed recursively, each time for all points $l$ of the rod.\n",
    "\n",
    "**Compute** the approximate solution to the 1D heat equation with the FTCS scheme, using the following parameters:\n",
    "* $\\alpha = 0.1$\n",
    "* $L=1$\n",
    "* $u_0^n = u_N^n = 0$ (fixed boundary conditions)\n",
    "* $\\Delta t = 10^{-4}$\n",
    "* $N = 100$\n",
    "\n",
    "Initialise the temperature distribution with random values between -1 and 1, and run the simulation for 1000 time steps. Plot the solution dynamically, at every iteration (or every $m$th iteration, with your choice of $m$) in the loop, to visualise how the temperature distribution changes over time along the rod -- [this](https://stackoverflow.com/a/39853938) may be helpful.\n",
    "\n",
    "\n",
    "### Further investigation\n",
    "\n",
    "Here are some ideas for further investigation -- you absolutely don't need to try all of them, and you could also try something I haven't mentioned here.\n",
    "\n",
    "#### Different parameters\n",
    "\n",
    "Try different initial distributions, and different sets of boundary conditions. You could have, for instance, zero temperature at one end, and a fixed, positive temperature at the other -- find out what the temperature distribution settles to. Try starting with a random temperature distribution, or a distribution with one or more peaks, where the rod would have been pre-heated at specific locations. Try different materials, with different values of $\\alpha$.\n",
    "\n",
    "You can also include point heat sources, distributed sources, and even moving sources -- in this case, you could use linear interpolation to \"spread\" the source between two grid points, depending on the position $x_s(n\\Delta t)$ of the source at time step $n$. Some of the resources listed below detail how to include source terms.\n",
    "\n",
    "#### Stability condition\n",
    "\n",
    "A finite difference scheme is **stable** if it has no solutions which grow exponentially over time. This is to say that *all* discrete solutions of the form $u_l^n = e^{n\\lambda \\Delta t}e^{i\\beta \\Delta x}$ must satisfy $\\mathrm{Re}(\\lambda)\\leq 0$ ($\\lambda \\in \\mathbb{C}, \\beta \\in \\mathbb{R}, i = \\sqrt{-1}$). Substituting this test solution into the scheme and enforcing the condition on $\\lambda$ will lead to an inequality relating $\\alpha$, $\\Delta t$, and $\\Delta x$ -- this is the **stability condition**. Find it, and verify it by running simulations with parameter values which violate it -- the solution should explode.\n",
    "\n",
    "#### Other schemes\n",
    "\n",
    "Other finite difference operators may be derived to approximate $\\frac{\\partial u}{\\partial t}$ at different orders of accuracy, by using Taylor series expansions of $u$ around $t_0$ truncated at different orders. For example, the *backwards* temporal difference is given by\n",
    "\n",
    "$$\n",
    "\\frac{\\partial u}{\\partial t}(x, t_0) \\approx \\frac{u(x, t_0) - u(x, t_0 - \\Delta t)}{\\Delta t}\n",
    "\\approx \\frac{u_l^n - u_l^{n-1}}{\\Delta t}.\n",
    "$$\n",
    "\n",
    "The backward-time, centred-space (BTCS) scheme is therefore given by:\n",
    "\n",
    "$$\n",
    "\\frac{u_l^n - u_l^{n-1}}{\\Delta t} = \\frac{u_{l-1}^n - 2u_l^n + u_{l+1}^n}{(\\Delta x)^2}.\n",
    "$$\n",
    "\n",
    "Another scheme, called the *Crank-Nicolson (CN) scheme*, relies on discretising the temporal derivative using the trapezoid rule, and is given by\n",
    "\n",
    "$$\n",
    "\\frac{u_l^{n+1} - u_l^n}{\\Delta t} = \\frac{1}{2}\\left(\n",
    "\\frac{u_{l-1}^{n+1} - 2u_l^{n+1} + u_{l+1}^{n+1}}{(\\Delta x)^2} +\n",
    "\\frac{u_{l-1}^n - 2u_l^n + u_{l+1}^n}{(\\Delta x)^2}\\right).\n",
    "$$\n",
    "\n",
    "These two schemes are *unconditionally stable* (you could try to prove this). However, they are *implicit*: if you try to rearrange them into a temporal recursion, you will see that the unknowns to solve for at each time step depend on each other. The way to compute the temperature distribution at the next time step is therefore to write the recursion as a linear system with $N$ equations and $N$ unknowns, and solve it using, for example, `np.linalg.solve()`. The unknowns for the BTCS scheme are the $u_l^n, l \\in \\{0, \\dots, N\\}$, and the unknowns for the CN scheme are the $u_l^{n+1}, l \\in \\{0, \\dots, N\\}$.\n",
    "\n",
    "#### Other PDEs\n",
    "\n",
    "It should be *relatively* straightforward to extend this to the 2D heat equation, to simulate the evolution of the temperature distribution over, say, a rectangular plate. You could also try to compute solutions to the wave equation, for instance, in 1D or 2D.\n",
    "\n",
    "### Resources\n",
    "\n",
    "* [*Finite Difference Schemes and Partial Differential Equations*, J. Strikwerda](https://doi.org/10.1137/1.9780898717938) -- a great textbook, available online through the library. In particular, Section 6.3 presents a number of finite difference schemes for the 1D heat equation.\n",
    "* [Solving the Heat, Laplace and Wave equations using finite difference methods](https://www.math.ubc.ca/~peirce/M257_316_2012_Lecture_8.pdf) -- lecture notes by Prof. Anthony Peirce, University of British Columbia\n",
    "* [Thermal diffusivity of different materials](https://en.wikipedia.org/wiki/Thermal_diffusivity)\n",
    "* [The diffusion equation](https://warwick.ac.uk/fac/cross_fac/complexity/study/msc_and_phd/co906/co906online/lecturenotes_2009/chap3.pdf) -- lecture notes by Dr Colm Connaughton at Warwick University\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project C: Newton and Broyden's methods for nonlinear systems\n",
    "\n",
    "Consider the following system of $n$ nonlinear equations in $n$ independent variables\n",
    "\n",
    "$$\n",
    "\\mathbf{f}(\\mathbf{x}) = \\mathbf{0},\n",
    "$$\n",
    "\n",
    "where $\\mathbf{x}\\in \\mathbb{R}^n$, and $\\mathbf{f}: \\mathbb{R}^n \\to \\mathbb{R}^n$ is a nonlinear vector-valued function. Suppose that $\\mathbf{f}$ is differentiable, and that this system has a unique solution $\\mathbf{x}^\\star \\in \\mathbb{R}^n$.\n",
    "\n",
    "In this project, you will implement two *iterative* methods to compute an approximate solution to a nonlinear system: **Newton's method** and **Broyden's method**. For both these methods, the idea is to start with an initial guess $\\mathbf{x}^{(0)}$, and iteratively refine this guess, until convergence to the solution is achieved.\n",
    "\n",
    "Both methods will be used to compute the solution to the following system:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    f_1(\\mathbf{x}) &= 2x_1 - x_2 + \\frac{a^2}{2} (x_1 + a + 1)^3, \\nonumber \\\\\n",
    "    f_i(\\mathbf{x}) &= 2x_i - x_{i-1} - x_{i+1} + \\frac{a^2}{2} (x_i + ia + 1)^3, \\qquad \\qquad i=2,\\ldots,n-1, \\\\\n",
    "    f_n(\\mathbf{x}) &= 2x_n - x_{n-1} + \\frac{a^2}{2} (x_n + na + 1)^3,\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "where $a = \\frac{1}{n+1}$.\n",
    "\n",
    "### Newton's method\n",
    "\n",
    "The next guess $\\mathbf{x}^{(k+1)}$ in Newton's method is computed as\n",
    "\n",
    "$$\n",
    "\\mathbf{x}^{(k+1)} = \\mathbf{x}^{(k)} - \\big(\\mathbf{J}^{(k)}\\big)^{-1} \\mathbf{f}(\\mathbf{x}^{(k)}),\n",
    "$$\n",
    "\n",
    "where $\\mathbf{J}^{(k)}$ is the Jacobian matrix of $\\mathbf{f}(\\mathbf{x}^{(k)})$. At each iteration, Newton's method therefore requires evaluating the function $\\mathbf{f}(\\mathbf{x}^{(k)})$, the Jacobian matrix $\\mathbf{J}^{(k)}$, and solving a linear system.\n",
    "\n",
    "**Compute** the solution of the test system given above using Newton's method, with the initial guess\n",
    "\n",
    "$$\n",
    "x_i^{(0)} = ia(ia - 1), \\qquad i=1,\\ldots,n,\n",
    "$$\n",
    "\n",
    "for different values of $n$ (up to $n \\sim 2000$). Use functionality provided by [the `time` module](https://docs.python.org/3/library/time.html#time.time) to find out the most computationally expensive operations in each iteration, and report your findings.\n",
    "\n",
    "### Broyden's method\n",
    "\n",
    "A class of algorithms, called *quasi-Newton methods*, take Newton's method as a starting point. In order to reduce computational cost, these methods approximate the Jacobian matrix instead of evaluating it at every iteration. They are particularly useful when the Jacobian is computationally expensive to evaluate.\n",
    "\n",
    "The Broyden step substitutes the Jacobian matrix in the Newton step with an approximation $\\tilde{\\mathbf{J}}^{(k)}$, computed iteratively from $\\tilde{\\mathbf{J}}^{(k-1)}$:\n",
    "\n",
    "$$\n",
    "\\tilde{\\mathbf{J}}^{(k)} =\n",
    "\\tilde{\\mathbf{J}}^{(k-1)} +\n",
    "\\left( \\mathbf{y}^{(k)} - \\tilde{\\mathbf{J}}^{(k-1)} \\mathbf{h}^{(k)} \\right)\n",
    "\\frac{{\\mathbf{h}^{(k)}}^{\\mathsf{T}}}{\\|\\mathbf{h}^{(k)}\\|_2^2},\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "\\mathbf{y}^{(k)} = \\mathbf{f}(\\mathbf{x}^{(k)}) - \\mathbf{f}(\\mathbf{x}^{(k-1)}), \\qquad\n",
    "\\mathbf{h}^{(k)} = \\mathbf{x}^{(k)} - \\mathbf{x}^{(k-1)}.\n",
    "$$\n",
    "\n",
    "An important thing to note here is that $\\tilde{\\mathbf{J}}^{(k)}$ is a **rank-1 update** of $\\tilde{\\mathbf{J}}^{(k-1)}$, and therefore its *inverse* can also be updated iteratively, using the *Sherman-Morrison formula* -- see the Resources linked below.\n",
    "\n",
    "**Compute** the solution of the test system using Broyden's method and the Sherman-Morrison formula. You will first need to initialise $(\\tilde{\\mathbf{J}}^{(0)})^{-1} = (\\mathbf{J}^{(0)})^{-1}$ explicitly.\n",
    "\n",
    "Use different values of $n$, up to $n \\sim 2000$, and measure computation times as before.\n",
    "\n",
    "### Further investigation\n",
    "\n",
    "Try both methods on different nonlinear systems, perhaps systems you have previously seen in your studies or in your research.\n",
    "\n",
    "Investigate and report the convergence properties of both methods, using different systems and different initial guesses.\n",
    "\n",
    "There is plenty of recent literature on modified and improved Newton and Broyden methods -- you could try implementing one of these.\n",
    "\n",
    "### Resources\n",
    "\n",
    "* [Solving nonlinear equations with Newton's method, C.T. Kelley](https://epubs.siam.org/doi/abs/10.1137/1.9780898718898.ch4) -- Chapter 4: Broyden's method\n",
    "* [Sherman-Morrison-Woodbury](https://www.cs.cornell.edu/~bindel/class/cs6210-f09/lec12.pdf) -- lecture notes by David Bindel, Cornell University\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project D: Polynomial least squares\n",
    "\n",
    "Suppose we have a set of data points $\\{x^{(i)},y^{(i)}\\}_{i=1,\\ldots,m}$. The goal is to fit a a polynomial of degree $(n-1)$ to the data, with coefficients $b_j, j=0,\\ldots,n-1$, of the form\n",
    "\n",
    "$$\n",
    "y = b_0 + x b_1 + x^2 b_2 + \\ldots + x^{n-1} b_{n-1}\n",
    "= \\sum_{j=0}^{n-1} x^j b_j.\n",
    "$$\n",
    "\n",
    "When $m > n$, the polynomial coefficients may be estimated so that the sum of squared residuals between the LHS and the RHS is minimised over the set of data points. The polynomial may be written in matrix-vector form, for all data points, as an overdetermined system:\n",
    "\n",
    "$$\n",
    "\\mathbf{y} = \\mathbf{Xb},\n",
    "$$\n",
    "\n",
    "where $\\mathbf{X}$ is the Vandermonde matrix $\\mathbf{X}\\in \\mathbb{R}^{m\\times n}$, with elements $\\mathbf{X}_{ij} = (x^{(i)})^{j-1}$ for $j=1,\\ldots,n$. This system has no exact solution -- however, there exists one set of coefficients $\\mathbf{b} \\in \\mathbb{R}^n$ which minimises $\\|\\mathbf{Xb} - \\mathbf{y}\\|_2^2$, the sum of squared residuals. It can be shown that this minimiser solves the **normal equations**\n",
    "\n",
    "$$\n",
    "\\mathbf{X}^T\\mathbf{Xb} = \\mathbf{X}^T \\mathbf{y}\n",
    "\\qquad \\Rightarrow \\qquad\n",
    "\\mathbf{b} = \\mathbf{X}^{\\dagger}\\mathbf{y}, \\quad \\text{where }\n",
    "\\mathbf{X}^{\\dagger} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T.\n",
    "$$\n",
    "\n",
    "The matrix $\\mathbf{X}^{\\dagger} \\in \\mathbb{R}^{n\\times n}$ is the *Moore-Penrose pseudo-inverse* of $\\mathbf{X}$.\n",
    "\n",
    "You can generate artificial data using a similar process as that seen in W4 to generate the noisy sinusoid -- use some polynomial as an underlying function instead, and try to estimate the coefficients from the noisy data by solving the normal equations with, e.g., `np.linalg.solve()`.\n",
    "\n",
    "Try polynomials of different degrees, with coefficients of different magnitudes, with more or less noisy data.\n",
    "\n",
    "If you recall one of the past worksheet problems, you may have noticed that the matrix $\\mathbf{X}^T\\mathbf{X}$ is very badly conditioned, which is likely to introduce very large numerical errors when solving the normal equations. There is a way to avoid having to form and invert this matrix: compute and use the **singular value decomposition** (SVD) of $\\mathbf{X}$ (see lecture notes linked in Resources).\n",
    "\n",
    "Finally, a common issue with polynomial regression is **overfitting** -- this occurs when a model is complex enough to correspond very closely to the particular set of data points you are training it with, but does not generalise well, and will not be able to predict future observations. Instead of capturing the relationship between input and output in a dataset, an overfitted model describes the statistical noise in the data.\n",
    "\n",
    "A method to avoid overfitting is **regularisation** -- here, we will look at a special case of Tikhonov regularisation (a.k.a. ridge regression for the statisticians in the audience). The regularised normal equations are given by\n",
    "\n",
    "$$\n",
    "(\\mathbf{X}^T\\mathbf{X} + \\mu \\mathbf{I})\\mathbf{b} = \\mathbf{X}^T\\mathbf{y},\n",
    "$$\n",
    "\n",
    "where $\\mu>0$ is a regularisation parameter. The idea is to introduce a penalty term, so that the polynomial coefficients $\\mathbf{b}$ (particularly the high-order coefficients) remain relatively small.\n",
    "\n",
    "Rewriting the regularised normal equations using the SVD of $\\mathbf{X}$ leads to a simple, explicitly computed solution $\\mathbf{b}$. Try implementing this with different values of $\\mu$, and try to find the best value for a given problem, large enough that overfitting is avoided, but small enough that the model isn't underfitted.\n",
    "\n",
    "### Further investigation\n",
    "\n",
    "This is straightforwardly generalisable to the multivariate case, when the data points have more than one attribute $x$. Try to apply the same method to perform polynomial regression on some of the datasets linked in the resources (also see the Project A description).\n",
    "\n",
    "Versions of polynomial regression are implemented in Numpy (`np.polyfit`), scikit-learn, and many other modules. Compare your results with some of these existing implementations.\n",
    "\n",
    "Polynomial regression is still a type of multiple *linear* regression -- the model is a linear function of the unknown polynomial coefficients. More elaborate regressions can be performed by using other basis functions than polynomials of $x$ -- you can read about these in the linked resources, and try to implement some of them to perform regression on different datasets.\n",
    "\n",
    "### Resources\n",
    "\n",
    "* [*Elements of Statistical Learning*, by Hastie, Tibshirani, and Friedman](https://web.stanford.edu/~hastie/ElemStatLearn/) -- Chapter 5: Basis expansions and regularization\n",
    "* [Least squares, pseudo-inverse, and SVD](http://www.sci.utah.edu/~gerig/CS6640-F2012/Materials/pseudoinverse-cis61009sl10.pdf) -- lecture notes by Guido Gerig, New York University\n",
    "* [Polynomial regression: extending linear models with basis functions](https://scikit-learn.org/stable/modules/linear_model.html#polynomial-regression-extending-linear-models-with-basis-functions) -- scikit-learn documentation\n",
    "* [Dataset archive](https://archive.ics.uci.edu/ml/datasets.php)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project E: Loss of significance in floating-point arithmetic\n",
    "\n",
    "In this project, you will explore and demonstrate several issues related to *loss of significance* when using floating-point numbers. You will investigate at least two example problems where numerical error plays a significant role; when alternative solutions exist to minimise these issues, you will demonstrate these.\n",
    "\n",
    "A few example problems to investigate could be (but are not limited to):\n",
    "* The quadratic formula (a classic!)\n",
    "* Heron's formula for the area of needle triangles\n",
    "* Small step sizes in finite difference approximations\n",
    "* Solving the linear system $Ax=b$ when $A$ is ill-conditioned\n",
    "* Propagation of error in large sums\n",
    "\n",
    "### Resources\n",
    "\n",
    "* [What Every Computer Scientist Should Know About Floating-Point Arithmetic](https://docs.oracle.com/cd/E19957-01/806-3568/ncg_goldberg.html) -- I have linked this resource in one of the worksheets. You should be able to find plenty of information there.\n",
    "* [Loss of Significance - The loss of numerical accuracy in computer calculation](https://zipcon.net/~swhite/docs/math/loss_of_significance.html)\n",
    "* [Miscalculating Area and Angles of a Needle-like Triangle, W. Kahan](https://people.eecs.berkeley.edu/~wkahan/Triangle.pdf)\n",
    "* [Ill-conditioned systems](http://engrwww.usask.ca/classes/EE/840/notes/ILL_Conditioned%20Systems.pdf)\n",
    "* [Ill-conditioned matrices](http://www.cs.uleth.ca/~holzmann/notes/illconditioned.pdf)\n",
    "* [How and How Not to Sum Floating-Point Numbers](http://www.phys.uconn.edu/~rozman/Courses/P2200_11F/downloads/sum-howto.pdf)\n",
    "* [Error and computer arithmetic](http://www.math.pitt.edu/~trenchea/math1070/MATH1070_2_Error_and_Computer_Arithmetic.pdf)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project F: Writing a Python tutorial\n",
    "\n",
    "* ***This project topic is highly recommended for anyone involved or interested in teaching.***\n",
    "\n",
    "In this project, you will write a **short worksheet** as a Jupyter notebook.\n",
    "Your target audience is an undergraduate mathematics student, who has perhaps taken an introductory Python programming course with scope similar to this one, with little or no previous programming experience. You can assume knowledge of any or all the content which was covered in our course.\n",
    "\n",
    "You will choose a specific topic related to Python programming and/or its applications in mathematical computing, and write a short tutorial to present the topic, including **interactive coding examples**, and references/links to the relevant parts of the **documentation**. You can choose to present some of the coding examples as short practice questions throughout the tutorial, if you like.\n",
    "\n",
    "Your tutorial should finish with **one practice problem with three questions**. Your problem should require some amount of *reflection and creativity* from the student -- for instance, questions similar in tone to the shorter practice \"Exercises\" we have seen in the worksheets are probably not sufficient. Think carefully about how to structure and order your questions.\n",
    "\n",
    "Your problem should assess the student's understanding of the material you have presented. You will also write **model solutions** to the questions, explained in detail and well-commented.\n",
    "\n",
    "As a guideline for length, your tutorial should cover an amount of content roughly equivalent to *one* of the worksheets you have seen on this course (only one of the 4 notebooks you had both weeks). However, don't worry too much about your worksheet being too short -- it is better to focus on little content, but explain it in detail and have well-designed examples.\n",
    "\n",
    "Some example topics on Python programming which you might choose to focus on could be:\n",
    "* [List (and other) comprehensions](https://docs.python.org/3/tutorial/datastructures.html#list-comprehensions)\n",
    "* [Classes in Python: creating custom types](https://docs.python.org/3/tutorial/classes.html)\n",
    "* [Reading and writing files](https://docs.python.org/3/tutorial/inputoutput.html#reading-and-writing-files)\n",
    "* [Generators](https://wiki.python.org/moin/Generators)\n",
    "* [Shallow and deep copy](https://docs.python.org/3.7/library/copy.html)\n",
    "\n",
    "If you want to focus on applications to mathematical computing, you may wish to take the other project topics as inspiration for your tutorial, or to choose another topic entirely. You could also take one of the topics we *have* seen in the course, and present it using a different approach.\n",
    "\n",
    "After the end of your worksheet, include a **short section** to *briefly* explain how you designed your tutorial, and how you chose to structure your questions. You will need to consult the documentation, (online) textbooks, and/or other educational resources to help you design your lesson -- this final section should also be the place where you include **references** to any such material which you have used."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
